[toc]

# :nerd_face: 大模型

## :hammer_and_wrench: AI工具

### :paintbrush: 绘画

1. 非猪AI：免费，集文本、绘画、视频、办公、聊天、编程、插件、Al模型下载、AI开发框架等于一体。 功能超级强大https://feizhuke.com/sites/ministerai.html
2. MinisterAl：国内免费，改写润色、多人协作、文本校对等功能https://chat.53site.com/home
3. 海艺Al：免费易用的Al绘画工具，超强的Al绘画网站，超高的出图质量和速度，Al绘画，设计https://www.seart.me/zhCN
4. photoroom：免费的AI抠图工具，无需注册，英文网站https://www.photoroom.com/tools/background-remover
5. 堆友AI反应堆：阿里旗下，暂时免费,需要消耗堆豆，功强大，可-键生成绘画https://d.design/ai
6. WHEE：美图旗下产品，需要注册，文生图、图生图、风格模型训练、AI模特、 AI扩图、 Al 改图等，可一键创作同款或应用模型https://www.whee.com/?channel=whllbd13&bd_vid=7231390320456013730

### :headphones: 音频

1. 网易天音：网易旗下一站式AI音乐创作工具https://tianyin.music.163.com/#/
2. 讯飞智作：AI配音工具，文字转语音、语音合成等，Al虚拟主播，声音定制https://peiyin.xunfei.cn/?utm_source=ai-bot.cn
3. 魔音工坊：短视频/有声书配音平台，可多人协同https://www.moyin.com/?ref=ai-bot.cn

### :pen: AI写作

1. 秘塔写作猫：国内免费，集AI写作、多人协作、文本校对、改写润色、自动配图等功能为一体https://xiezuocat.com/?s=cbdaq
2. COPY AI智能创作：小红书文案、跨境电商文案以及短视频文案，智能问答，营销文案、视频文案https://www.copyai.top/index
3. Jasper：国外网站，需要谷歌账号，7天免费试用，之后收费https://ww.jasper.ai/
4. 讯飞星火：类chatgpt工具，需注册，包含了星火APP、星火API、 星火助手、内容运营大师等产品https://xinghuo.xfyun.cn/
5. 通义千问：当前限时免费，覆盖文本回答、图片理解、文章解析等https://tongvi.aliyun.com/

### :clapper: AI视频生成

1. 闪剪-AI数字人-款AI智能视频剪辑工具，可以快速将图片、文章、直播等内容通过分身数字人、AI智能剪辑等技术自动生成视频。并提供丰富的视频模板、AI配音员等素材。https://shanjian.ty/?inviteld=64a63fb8288578003ab7635b
2. 一帧秒创AI视频创作平台，图文转视频，一键输出 https://aigc.yizhentv.com/?_f=yipoo
3. Pika Labs可以根据描述词生成视频。PikaLabs能够生成非常流畅的视频，甚至可以用于广告和电影制作。https://ww.yjpoo.com/index.php?m=home&c=jump&a=jump&aid=942
4. Reface.ai换脸软件一个人工智能AI换脸应用，可以让你在视频、GIF和表情包中进行换脸操作，比如将电影主角换成你自己的脸，体验一把当明星的感觉。https://wipipo.com/index.php?m=home&c=Jump&a=jump&aid=912

### :muscle: 比较强的AI工具

1. GPT-4
2. 文心一言
3. Midjourney
4. 文心一格（国内Midjourney）
5. Bing AI

### :wrench: 其他AI工具

1. Poe：

   - 由Quora (海外问答平台，类似国内知乎)开发，有APP版本， 支持跨端使用。主要亮点在于集成了Chat GPT、GPT-4、 Claude+、 Claude、 Dragonfly 等模型，同时支持用户自建Chatbot。不同语言模型回复效果有差异， 适合需要调用多种大语言模型的用户。

   - Dragonfly擅长给出较短的回答，并擅长在输入中给出示例时遵循指示。

   - Claude更擅长创造性回复，配合Poe中的提问引导，非常适合在查阅资料时使用，有时能够给出超越直接使用Chat GPT时的体验(但和Chat GPT-样，Claude 也时常会给出一些错误回复，一些问题我会尝试在两个模型中都问一 遍提升信息准确性)。此外支持分享用户和模型的对话内容。但GPT-4、Claude+ 产品需要付费订阅使用。
     访问地址: https://poe.com/

2. Prompt Perfect ：
   能够根据输入的Prompt进行优化，并且能给出模型在Prompt优化前后给出的结果对比。访问地址: https://promptperfectjina.ai/arena

3. coze：https://coze.cn
   Coze允许你不管是否懂编程，都能快速打造出各种各样的聊天机器人、智能体、Al应用和插件。

## :open_hands: 开源项目

#### Auto-GPT

Auto-GPT是一个能自主迭代、自主迭代(长时记忆)、自我提示且联网查询的新的 GPT框架，它不需要你与ChatGPT在多轮对话中让ChatGPT逐步完成你的任务，而是最少只需要在第一轮对话中输入需求，Auto-GPT 就能自己分解任务去完成，且完成度更高。能预感出这就是将来的工作方式，后期Auto-GPT应该能调用更多的工具和插件，甚至是桌面应用，这将完全解放双手。

开源地址: https://github.com/Significant-Gravitas/AutoGPT

#### notesGPT

这是一个免费、开源的语音记事应用程序，它在开源一-周就吸引了35000名访问者、7000名用户，1.4k的star。 它可以录制语音便笺，使用Whisper进行转录，并通过Together使用Mixtral提取操作项并在操作项视图中显示它们。它还完全开源，并配备了身份验证、存储、向量搜索、操作项，并且完全响应移动设备，以便于使用。
开源地址: https://github.com/Nutlope/notesGPT

#### ChatGPT-Next-Web

NextChat (ChatGPT Next Web)可以一键免费部署你的跨平台私人ChatGPT应用，支持GPT3，GPT4 & Gemini Pro模型。其实就是目前非常流行的Al助手或者说私人部署ChatGPT,这个项目是由大神Yidadaa最初在github.上发布的一款ChatGPT应用。现在经过一段时间的发展，已经为了同类产品里的翘楚。

目前产品的主要功能包括:

- 在1分钟内使用Vercel免费一键部署
- 提供体积极小(~5MB) 的跨平台客户端(Linux/Windows/MacOS)
- 完整的Markdown支持: LaTex 公式、Mermaid 流程图、代码高亮等等
- 精心设计的UI,响应式设计，支持深色模式，支持PWA
- 极快的首屏加载速度(~ 100kb)支持流式响应
- 隐私安全，所有数据保存在用户浏览器本地
- 预制角色功能(面具)，方便地创建、 分享和调试你的个性化对话元
- 海量的内置prompt列表
- 自动压缩上下文聊天记录，在节省Token的同时支持超长对话
- 多国语言支持
- 支持绑定域名

项目地址：https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web/releases

#### chatgpt-on-wechat

chatgpt-on-wechat是一个前沿的开源项目， 是个基于大模型的智能对话机器人。 支持微信、企业微信、公众号、飞书、 钉钉接入。
旨在将OpenAI的先进自然语言处理模型ChatGPT与微信等平台深度融合，实现高度智能化和人性化的文本交互服务，可选择GPT3.5/GPT4.0/Claude/文心一 言/讯飞星火通义千问/Gemini/LinkAI/ZhipuAl,能处理文本、语音和图片，通过插件访问操作系统和互联网等外部资源，支持基于自有知识库定制企业AI应用。
该项目聚焦于提升微信用户的聊天体验，通过精准理解用户意图并生成连贯、逻辑清晰且富有创意的文本回复。

项目地址：https://github.com/zhayujie/chatgpt-on-wechat

## :page_with_curl: 精选文章

###### 为什么号称模拟世界的Sora,处理不好些简单的物理规律? 

https://mp.weixin.qq.com/s/HSZMbiFuNvTmBv26csZFGg

###### 谷歌:开源Gemma可商用，性能超过Lama 2!

https://mp.weixin.qq.com/s/7s9XmBveQh9X2uj3rseAjQ

###### 好文分享: ChatGPT中常说的token到底是什么?

https://juejin.cn/post/7321989698027798591?utm_source=gold.%20browser_extension

###### 6700字大模型扫盲系列

https://mp.weixin.qq.com/s/ezb-4I2JLK31fUcvgMM_Lg

###### 清华、哈工大把大模型压缩到了1bit

论文标题: OneBit: Towards Extremely Low-bit Large Language Models

###### 杨立昆团队提出图像世界模型:在视觉表征学习中学习和利用世界模型

https://cz5waila03cyo0tux1owpyofgoryroob.aminer.cn/0C/81/91/0C8191C9DCC854569A22A6F7BC77C2AD.pdf

###### 推动全球AI大模型发展的重量级开源大模型Grok-1到底是什么?

https://mp.weixin.qq.com/s/Oqz9Q6YRSEdWnvf8Qq_JDQ

###### 工作中怎么用ChatGPT?如何高效Prompt?

https://juejin.cn/post/7222823687837941797?utm_source=gold_browser_extension



## :question: 面试题

#### :package:目前主流的大模型体系有哪些?

目前主流的开源大模型体系有GPT系列、BERT 系列、Llama系列，还有国内ChatGLM系列、Baichuan系列等。

- GPT系列:由OpenAl发布的基于Transformer架构的语言模型，包括GPT-1、GPT-2、 GPT-3、 ChatGPT 等。在自然对话、文本生成等任务上表现出色。
- BERT系列:由Google发布的预训练语言模型，例如BERT、RoBERTa 等。在各种NLP任务中都有良好的效果。
- Llama系列:由Meta AI发布的一系列大型自然语言模型(LLM) ，在自然语言处理任务中表现出色。Llama-1是第一 个版本，具有4个模型规模: 7B、 13B、 33B和65B参数。Llama-2是下一代，分为7B、13B和70B参数。
- XLNet:由Google研发，优化了Transformer和BERT,提升了表征学习能力。
- ERNIE:由百度研发，融合了结构化知识，对中文下游任务效果良好。
- T5: Google 提出的"文本到文本'转换范式，适用于各种NLP下游任务。
- ChatGLM系列:由智谱AI和清华大学KEG实验室联合发布的对话预训练模型。其中，ChatGLM3-6B是ChatGLM系列中的开源模型。ChatGLM支持多种语言， 并具有高效的性能，可以快速地处理大量的文本数据。
- Baichuan系列:由百川智能发布的一系列大型语言模型。其中Baichuan-13B 是该系列中的一个开源模型，具有130亿参数。 Baichuan-13B 在性能方面表现出色，特别是在数学、代码、安全、逻辑推理和语义理解等方面。

#### :package:什么是多模态，多模态中常见的SOTA模型有哪些?

多模态是指涉及多种模态(如图像、文本、音频、视频等)的数据处理和分析。多模态学习是一种利用多种模态的数据来进行机器学习的方法，它可以挖掘不同模态之间的关联性和互补性，提高数据的表达能力和理解能力。多模态学习的应用场景非常广泛，例如图像描述、视觉问答、语音识别、跨模态检索等。

常见的SOTA模型有:

- Vision Transformer (ViT):这是种将自注意力机制引入计算机视觉领域的模型，通过将图像划分为图像补丁并应用Transformer模型，实现了在图像分类和目标检测等任务上的出色表现。
- CLIP (Contrastive Language-lmage Pre-training): 一种利用海量从网络上搜集的图像文本对进行对比学习的模型，使用一个图像编码器和一个文本编码器分别对图像和文本独立编码，再以对比学习为优化目标训练模型。CLIP模型在零样本图像分类任务，以及图文匹配和检索等问题上取得了非常好的效果。
- CoCa (Contrastive Captioners):这是种融合了单编码器、双编码器和编码器解码器三种结构的多模态模型，既能生成图像侧和文本侧独立的表示，又能进行更深层次的图像、文本信息融合以及文本生成。CoCa在图像分类、图文检索、看图说话、VQA等多个任务上都取得了SOTA效果。

 补充：https://wwlilbili.com./read/cv24894357/

#### :package: 了解LoRA么?

- LoRA (Low-Rank Adaptation) : 在机器学习领域，特别是自然语言处理中,LoRA是"Low-Rank Adaptation"的简称，是一种用于微调大型预训练模型的技术。

- 基本思想:在不改变原始预训练权重的基础上，通过引入可训练的低秩分解矩阵来调整模型参数，以适应特定任务或领域。
- 这种方法的优势:相较于全模型微调，它仅更新少数参数，因此可以显著减少计算资源消开耗和潜在的过拟合风险，同时保留了预训练模型的泛化能力。

#### :package:encoder-only, decoder-only, encoder-decoder的区别?

在深度学习和自然语言处理(NLP) 领域中，Encoder-Only、 Decoder-Only以及Encoder-Decoder架构是三种不同类型的神经网络结构，它们各自设计用于不同的任务:

- Encoder-Only架构：
  - 定义与特点：这类模型仅包含编码器部分，主要用于从输入数据提取特征或表示。例如，在BERT (Bidirectional Encoder Representations from Transformers)中，它是一个双向Transformer编码器，被训练来理解文本上下文信息，并输出一个固定长度的向量表示，该表示包含了原始输入序列的丰富语义信息。
  - 用途：主要用于预训练模型，如BERT、RoBERTa等， 常用于各种下游任务的特征提取，比如分类、问答、命名实体识别等， 但不直接用于生成新的序列。
- Decoder-Only架构：
  - 定义与特点：解码器仅架构专注于从某种内部状态或先前生成的内容生成新的序列，通常用于自回归式预测任务，其中每个时刻的输出都依赖于前面生成的所有内容。
  - 优点：强大的序列生成能力，能够按顺序逐个生成连续的元素(如单词、字符)， 适用于诸如文本生成、自动摘要、对话系统等生成性任务。典型的Decoder-Only模型包括GPT系列(如GPT-3)。
- Encoder-Decoder架构：
  - 定义与特点：这种架构由两个主要部分组成:编码器和解码器。编码器负责将输入序列转换为压缩的中间表示，解码器则基于这个中间表示生成目标输出序列。这种结构非常适合翻译、摘要生成、图像描述等任务，需要理解和重构输入信息后生成新序列的任务。
  - 工作原理：编码器对源序列进行处理并生成上下文向量，解码器根据此上下文向量逐步生成目标序列。例如，经典的Seq2Seq (Sequence-to-Sequence) 模型和Transformer中的机器翻译模型就采用了这样的结构。
- 总结:
  Encoder-Only用于理解输入并生成其抽象表示，不涉及序列生成。
  Decoder-Only专门用于根据之前的信息自动生成新序列，不接收外部输入。
  Encoder-Decoder结合了两者的功能，首先对输入进行编码，然后基于编码结果解码生成新序列。

#### :package:介绍一下 Transformer?

Transformer是一 种由谷歌在2017年提出的深度学习模型， 主要用于自然语言处理(NLP)任务，特别是序列到序列(Sequence-to-Sequence) 的学习问题，如机器翻译、文本生成等。Transformer彻底改变 了之前基于循环神经网络(RNNs) 和长短期记忆网络(LSTMs)的序列建模范式，并且在性能上取得了显著提升。

- Transformer的核心创新点包括:

  1. 自注意力机制(Self-Attention Mechanism) : Transformer模型摒弃了传统RNN结构的时间依赖性，通过自注意力机制实现了对输入序列中任意两个位置之间的直接关联建模。每个词的位置可以同时关注整个句子中的其他所有词，计算它们之间的相关性得分，然后根据这些得分加权求和得到该位置的上下文向量表示。这种全局信息的捕获能力极大地提高了模型的表达力。
  2. 多头注意力(Multi-Head Attention) : Transformer进一 步将自注意力机制分解为多 个并行的“头部”，每个头部负责从不同角度对输入序列进行关注，从而增强了模型捕捉多种复杂依赖关系的能力。最后，各个头部的结果会拼接并经过线性变换后得到最终的注意力输出。
  3. 位置编码(Positional Encoding) :由于Transformer不再使用RNN那样的顺序处理方式，为了引入序列中词的位置信息，它采用了一种特殊的位置编码方法。这种方法对序列中的每个位置赋予一个特定的向量，这个向量的值与位置有关， 确保模型在处理时能够区分不同的词语顺序。
  4. 编码器解码器架构(Encoder-Decoder Architecture) : Transformer采用 了标准的编码器解码器结构，其中编码器负责理解输入序列，将其转换成高级语义表示;而解码器则依据编码器的输出并结合自身产生的隐状态逐步生成目标序列。在解码过程中，解码器还应用了自注意力机制以及一种称为"掩码”(Masking) 的技术来防止提前看 到未来要预测的部分。
  5. 残差连接(Residual Connections) : Transformer沿用 了ResNet中的残差连接设计，以解决随着网络层数加深带来的梯度消失或爆炸问题，有助于训练更深更复杂的模型。
  6. 层归一化(Layer Normalization) : Transformer使用层归一化而非批量归一化，这使得模型在小批量训练时也能获得良好的表现，并且有利于模型收敛。

  正是以上这些关键特性共同构成了Transformer强大的序列建模能力，使其成为现代NLP领域的基石模型，并被广泛应用于BERT、GPT系列、T5等多种预训练模型中，极大地推动了NLP技术的发展。

#### :package: 微调方法p-tuning v2的原理

P-Tuning V2在P-Tuning V1的基础上进行了下述改进:

1. 在每一层都加入了Prompts tokens作为输入， 而不是仅仅加在输入层，这与Prefix Tuning的做法相同。这样得到了更多可学习的参数，且更深层结构中的Prompt能给模型预测带来更直接的影响。
2. 去掉了重参数化的编码器。在P-tuning v2中，作者发现重参数化的改进很小，尤其是对于较小的模型，同时还会影响模型的表现。
3. 针对不同任务采用不同的提示长度。 提示长度在提示优化方法的超参数搜索中起着核心作用。在实验中，我们发现不同的理解任务通常用不同的提示长度来实现其最佳性能，这与Prefix-Tuning中的发现一致，不同的文本生成任务可能有不同的最佳提示长度。
4. 可选的多任务学习。 先在多任务的Prompt上进行预训练，然后再适配下游任务。一方面，连续提示的随机惯性给优化带来了困难，这可以通过更多的训练数据或与任务相关的无监督预训练来缓解:另一方面，连续提示是跨任务和数据集的特定任务知识的完美载体。

#### :package: 为什么现在的LLM都是Decoder only的架构?

1. 双向注意力可能存在的低秩问题。双向注意力带来的低秩问题会导致生成效果下降；
2. 在同等参数量、 同等推理成本下，Decoder-only架构很比另外两种框架具有更优的效果；
3. Decoder-only的zero-shot能力更强。

#### :package: RMSNorm比LayerNorm好在哪里？

简单来说就是，虽然 ==二者的时间复杂度一致== ， 但是RMSNorm比起LayerNorm确实 ==减少了减去均值以及加上bias的计算== ，这在目前大模型预训练的计算量下就能够体现出训练速度上的优势了，**并且RMSNorm在模型效果上的表现并不弱于LayerNorm**(LN取得成功的原因可能是缩放不变性，而不是平移不变性)，所以选择RMSNorm就很自然了，要注意RMSNorm也是LayerNorm的一种，以上提到的LayerNorm指的是最常见的形式。

#### :package: 文本生成的几大预训练任务?

1. GPT (Generative Pre-trained Transformer) 系列:包括GPT、GPT-2、 GPT-3等。这些模型使用Transformer架构进行预训练，在大规模语料上学习的语言模型，能够生成连贯、具有语义的文本。

2. BART (Bidirectional and Auto-Regressive 3) Transformer) : BART是一 种基于Transformer的生成式预训练模型。它通过自回归解码器实现文本生成，通过自编码器预训练目标来重构输入文本，能够生成流畅、连贯的文本。
3. T5 (Text-to-Text Transfer Transformer) : T5是一 种通用的文本生成模型，使用了编码器——解码器结构。它将不同的自然语言处理(NLP) 任务转换为文本到文本的转换任务，可用于机器翻译、摘要生成、问题回答等多种NLP任务。
4. XLNet: XLNet是一种基于Transformer架构的预训练模型，采用了自回归和自编码器的组合方式进行训练。它在语言建模任务上引入了全局的上下文信息，能够生成更加准确和连贯的文本。
5. 行UniLM (Unified Language Model) : UniLM是一 种多任务学习的预训练模型， 将不同的自然语言处理任务转化为统一的生成式任务。它可以用于文本摘要、问答系统、机器翻译等多种NLP任务。

#### :package: 对比学习负样本是否重要?负样本构造成本过高应该怎么解决?

对比学习中负样本的重要性取决于具体的任务和数据。负样本可以帮助模型学习到样本之间的区分度，从而提高模型的性能和泛化能力。然而，负样本的构造成本可能会较高，特别是在一些领域和任务中。

为了解决负样本构造成本过高的问题，可以考虑以下方法:

1. 降低负样本的构造成本：
   通过设计更高效的负样本生成算法或采样策略，减少负样本的构造成本。例如，可以利用数据增强技术生成合成的负样本，或者使用近似采样方法选择与正样本相似但不相同的负样本。
2. 确定关键负样本：
   根据具体任务的特点，可以重点关注一些关键的负样本， 而不是对所有负样本进行详细的构造。这样可以降低构造成本，同时仍然能够有效训练模型。
3. 迁移学习和预训练模型：
   利用预训练模型或迁移学习的方法，可以在其他领域或任务中利用已有的负样本构造成果，减少重复的负样本构造工作。